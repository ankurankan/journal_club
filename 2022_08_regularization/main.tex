\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usecolortheme{beaver}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{bm}
\usepackage[style=verbose, backend=biber]{biblatex}

\title{Partially Adaptive Regularized Multiple Regression Analysis for Estimating Linear Causal Effects}
\author{Hisayoshi Nanmo, Manabu Kuroki}
\date{}

\begin{document}
\maketitle

\begin{frame}
	\frametitle{Regularization in Linear Regression}
	\begin{itemize}
		\item Linear Regression Model: $ \bm{Y} = \bm{XB} + \bm{e} $
		\item OLS: $ \hat{B} = min_{B} (Y - XB)^2 $
		\item Ridge Regression (L2): $ \hat{B} = min_{B} (Y-XB)^2 +
			\lambda \sum B^2 $
		\item Lasso Regression (L1): $ \hat{B} = min_{B} (Y-XB)^2 +
			\lambda \sum \mid B \mid $
		\item Elastic net: $ \lambda_2 \sum \bm{B}^2 + \lambda_1 \sum
			\mid \bm{B} \mid $
		\item Used when we have high-dimensional scenarios or
			correlated covariates as estimates are bad for simple
			OLS.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Linear Regression in Causal Inferene}
	\begin{itemize}
		\item Given a DAG with the exposure ($ E $) and the outcome
			variable ($ O $).
		\item First step would be to find an adjustment set ($ A $).
		\item Assuming linearity in all relationships, we can then run
			a linear regression: $ O ~ E + A $ to estimate the
			treatment effect of $ E $ on $ O $.
		\item In the case of high dimensional adjustment set, we run
			into the same problem with OLS.
		\item But using any regularization scheme would also affect the
			estimate of the causal effect parameter which would
			lead to bias.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Proposed Soluation: Summary}
	\begin{itemize}
		\item Regularization methods for dealing with multicollinearity
			and high-dimensional datasets can lead to biased
			estimates for causal estimation.
		\item A new regularization term is introduced that does a
			``partial'' regularization on a set of selected
			covariates.
		\item Theoretical Results: Extend the collapsibility results to regularized
			regression analysis.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Discussion Points}
	\begin{itemize}
		\item Best strategies for selecting the confounding set, $
			\bm{W} $, and the set of possible confounders, $ \bm{Z}
			$.
		\item Choosing the optimal hyperparameters. Grid search for best 
			predictive performance or any other better strategies?
		\item Extending it to other statistical models like Generalized
			Linear Models, Generalized Estimating Equations, etc.
	\end{itemize}
\end{frame}

\end{document}
