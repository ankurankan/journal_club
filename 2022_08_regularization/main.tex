\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usecolortheme{beaver}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[style=verbose, backend=biber]{biblatex}
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}

\title{Partially Adaptive Regularized Multiple Regression Analysis for Estimating Linear Causal Effects}
\author{Hisayoshi Nanmo, Manabu Kuroki}
\date{}

\begin{document}
\maketitle

\begin{frame}
	\frametitle{Regularization in Linear Regression}
	\begin{itemize}
		\item Linear Regression Model: $ \bm{Y} = \bm{XB} + \bm{e} $
		\item OLS: $ \bm{\hat{B}} = min_{\bm{B}} (\bm{Y} - \bm{XB})^2 $
		\item OLS fails when have high-dimensional covariates or multicolinearity in covariates. Regularization can be used:
			\begin{itemize}
				\item Ridge Regression (L2): $ \bm{\hat{B}} = min_{\bm{B}} [(\bm{Y}-\bm{XB})^2 + \mathcolorbox{yellow}{\lambda \sum \bm{B}^2}] $
				\item Lasso Regression (L1): $ \bm{\hat{B}} = min_{\bm{B}} [(\bm{Y}-\bm{XB})^2 + \mathcolorbox{yellow}{\lambda \sum \mid \bm{B} \mid}] $
				\item Elastic net: $ \bm{\hat{B}} = min_{\bm{B}} [(\bm{Y} - \bm{XB})^2 + \mathcolorbox{yellow}{\lambda_2 \sum \bm{B}^2 + \lambda_1 \sum \mid \bm{B} \mid}] $
			\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Linear Regression in Causal Inferene}
	\begin{itemize}
		\item Given a DAG with the exposure ($ E $) and the outcome
			variable ($ O $).
		\item First step would be to find an adjustment set ($ A $).
		\item Assuming linearity in all relationships, we can then run
			a linear regression: $ O ~ E + A $ to estimate the
			treatment effect of $ E $ on $ O $.
		\item In the case of high dimensional adjustment set, we run
			into the same problem with OLS.
		\item But using any regularization scheme would also affect the
			estimate of the causal effect parameter which would
			lead to bias.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Contribution Summary}
	\begin{itemize}
		\item A new regularization term is introduced that can do a
			``partial'' regularization on a set of selected
			covariates.
		\item Theoretical Results: Extend the collapsibility results to
			regularized regression analysis.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Partial Regularization}
\end{frame}

\begin{frame}
	\frametitle{Collapsibility}
\end{frame}

\begin{frame}
	\frametitle{Numerical Results}
\end{frame}

\begin{frame}
	\frametitle{Discussion Points}
	\begin{itemize}
		\item Best strategies for selecting the confounding set, $
			\bm{W} $, and the set of possible confounders, $ \bm{Z}
			$.
		\item Choosing the optimal hyperparameters. Grid search for best 
			predictive performance or any other better strategies?
		\item Extending it to other statistical models like Generalized
			Linear Models, Generalized Estimating Equations, etc.
	\end{itemize}
\end{frame}

\end{document}
