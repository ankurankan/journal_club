\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usecolortheme{beaver}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{todonotes}
\usepackage{bm}

\def\ci{\perp\!\!\!\!\!\perp}

\newtheorem{proposition}{Proposition}

\begin{document}

\title{A Simple Unified Approach to Testing High-Dimensional Conditional
Independencies for Categorical and Ordinal Data}
\author {Ankur Ankan \and Johannes Textor}
\date{}
\maketitle

\begin{frame}
	\frametitle{Overview}
	\tableofcontents
\end{frame}

\section{Motivation}
\begin{frame}
	\frametitle{Motivation: Example DAG}
	\begin{figure}
		\centering
		\includegraphics[scale=0.6]{imgs/example_dag.png}
		\caption*{An example Directed Acyclic Graph (DAG)}
	\end{figure}
	\begin{itemize}
		\setlength\itemsep{1em}
		\item Random variables are represented using nodes.
		\item Directed edges represent direct causal link between variables.
		\item DAG structures imply Conditional Independencies (CI).
		\item In the example model: \newline
			\hspace*{20pt} $ \text{\emph{Pollution}} \ci \text{\emph{XRay}} | \text{\emph{Cancer}} $ \newline
			\hspace*{20pt} $ \text{\emph{XRay}} \ci \text{\emph{Breathing}} | \text{\emph{Cancer}} $
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Motivation: Model Testing}
	\begin{itemize}
		\setlength\itemsep{1em}
		\item In applied research, most of these models are made by hand
			based on domain knowledge.
		\item Implied CIs can be tested in the dataset to verify
			model structure.
		\item Is $ \text{\emph{Pollution}} \ci \text{\emph{XRay}} | \text{\emph{Cancer}} $ in dataset, $ D $ ?
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Motivation: Structure Learning}
	\begin{itemize}
		\setlength\itemsep{1em}
		\item CI implies that no direct causal link exists between the variables. \newline
		\hspace*{20pt} $ \text{\emph{Pollution}} \ci \text{\emph{XRay}} | \text{\emph{Cancer}} \implies \text{\emph{Pollution}} \not \rightarrow  \text{\emph{XRay}} $

		\item Constraint-Based structure learning algorithms like PC
			and FCI use these tests to determine model skeletons
			from data.
	\end{itemize}
\end{frame}

\section{Background}
\begin{frame}
	\frametitle{(Conditional) Independence Test}
	\begin{block}{Independence Test}
		Two random variables $ X $ and $ Y $ are independent,
		$ X \ci Y $ if and only if $ P(X, Y) = P(X) \cdot P(Y) $.
	\end{block}
	\vspace{1em}

	\begin{block}{Conditional Independence Test} Two random variables $ X $
		and $ Y $ and are said to be conditionally independent given $
		\bm{Z} $, $ X \ci Y | \bm{Z} $ if and only if for all $ z $
		with $ p(z) > 0 $, $ P(X, Y | Z=z) = P(X | Z=z) \cdot P(Y |
		Z=z) $
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{CI Testing is Difficult}
	\begin{itemize}
		\setlength\itemsep{1em}
		\item Normal non-conditional independence testing is easy.
		\item Conditional indpendence testing is a hard problem.
		\item For continous condiitonal variables, has been proven that no single 
		      test exists which has power against all datasets. \footnotemark
		\item But also well studied problem, and many approaches exist.
	\end{itemize}

	\footnotetext[1]{\footnotesize{Shah, Rajen D., and Jonas Peters. "The hardness of conditional independence testing and the generalised covariance measure." The Annals of Statistics 48.3 (2020): 1514-1538.}}
\end{frame}

\begin{frame}
	\frametitle{Main classes of tests}
	\begin{itemize}
		\setlength\itemsep{1em}
		\item Stratification based tests
		\item Variable Importance based tests
		\item Residulaization based tests
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Stratification Based Tests}
	\begin{itemize}
		\setlength\itemsep{1em}
		\item Most common type for discrete variables. Eg. chi-squared,
			mutual information etc. 
		\item Converts CI test into simple independence test by creating 
			stratum. 
			\todo[inline]{Insert a figure to make this clearer}
		\item Runs test on each of the stratum and then combine into a 
			single result.
		\item Problem: Looses power when number of conditonal variables
			are increased.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Variable Importance Tests}
	\begin{itemize}
		\item Compares two probablity models $\hat{p}(x | y, z) $ and $
			\hat{p}(x | z) $.
		\item If simpler model doesn't fit significantly worse, then $
			X \ci Y | Z $.
		\item SCCI is an example which compares the Kolmogorov
			complexity. Currently state-of-the-art for structure
			learning with discrete variables.
		\item Problems: Inherent asymmetry: The result of $ X \ci Y | Z
			$ can be different from $ Y \ci X | Z $.
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Residualization Based Tests}
	\begin{itemize}
		\item Example: partial correlation test.
		\item Fits two models $ \mathbb{E}[X | Z] $ and $ \mathbb{E}[Y | Z] $.
		\item Compute residuals $ R_{X|Z} $ and $ R_{Y|Z} $.
		\item Under CI, if $ \mathbb{E}[R_{X|Z}] = \mathbb{E}[R_{Y|Z}] = 0 $, the
			$ \mathbb{E}[R_{X|Z}R_{Y|Z}] = 0 $. \footnotemark
	\end{itemize}
	\footnotetext[1]{\footnotesize Daudin, J. J. "Partial association measures and an application to qualitative regression." Biometrika 67.3 (1980): 581-590.}
\end{frame}

\section{Proposed Method}
\begin{frame}
	\frametitle{Random Forest Based Test (RFT)}
	\begin{itemize}
		\item Residualization based approach.
		\item Uses random forest as the estimator.
		\item Uses Lee-Shepherd residuals.
		\item Gives a chi-square distributed test statistic.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Lee-Shepherd (LS) Residuals}
	Given a sample $ \bm{y} $ of $ Y $ and an estimate $ \hat{p}(y) $ of $ p(y) $,
	LS-Residual is defined as:
	$$ R_{y_i} = \hat{p}(Y < y_i) - \hat{p}(Y > y_i) $$
	\vspace{1em}

	For the binary case with $ Y \in {0, 1} $:
	$$ R_{y_i} = y_i - \hat{p}(Y = 1) $$
	\vspace{1em}

	For the conditional case for sample $ (\bm{y}|\bm{z})_i $,
	$$ R_{y_i | z_i} = \hat{p}(Y < y_i | Z=z_i) - \hat{p}(Y>y_i|Z=z_i) $$

\end{frame}

\begin{frame}
	\frametitle{Proposition 1}
	\begin{proposition}
	If $ X \ci Y | Z $ and $ \hat{p}(x|z) $ and $ \hat{p}(y|z) $ are asymptotically
	unbiased estimators of $ p(x|z) $ and $ p(y|z) $ respectively, then 
	$ \mathrm{Cov}(R_{x|z}, R_{y|z}) = 0 $.	
	\end{proposition}
	\vspace{1em}

	\begin{itemize}
		\item The residual property of LS-Residuals that $
			E[R_{x_i|z_i}] = E[R_{y_i|z_i}] = 0 $.
		\item From Daudin(1980), if residual expections are $ 0 $, if $
			X \ci Y | Z $ then residual covariance will be $ 0 $.
		\item This property allows us to use LS-Residuals for the residualization
			based tests.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{RFT: Both ordinal variables}
	$$ Q_1(\bm{x}, \bm{y}) = \frac{1}{n} \frac{(R_{\bm{x}} \cdot R_{\bm{y}})^2}{\bm{var}(R_{\bm{x}} R_{\bm{y}})} $$

	If $ X \ci Y | Z $, then asymptotically $ Q_1(\bm{x}, \bm{y}) \sim \chi^2(1) $.

	\begin{itemize}
		\item Square of sample mean divided by standard deviation.
		\item Under CI, the population mean is 0.
		\item Using CLT, the standardized sample mean is asymptotically
			standard normal.
		\item Square of this gives a chi-squared distribution with $ 1
			$ degree of freedom.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{RFT: One ordinal and one categorical}
	One-hot-encoding or Dummy Variables.

	$$ d = (R_{\mathbb{I}(\mathbf{x}=1)} \cdot R_{\mathbf{y}}, \, \ldots \ ,
		R_{\mathbb{I}(\mathbf{x}=k-1)} \cdot R_{\mathbf{y}}
	)$$

	$$ Q_2(\bm{x}, \bm{y}) = \frac{1}{n} (d \times \hat{\Sigma}_d^{-1} \times d^T) $$

	If $ X \ci Y | Z $, then asymptotically $ Q_1(\bm{x}, \bm{y}) \sim \chi^2(k-1) $.
	\begin{itemize}
		\item Under CI, each component of $ d $ is asymptotically normal.
		\item Hence, $ d $ is multivariate gaussian.
		\item $ \sqrt(Q_2(\bm{x}, \bm{y})) $ is multivariate standard normal.
		\item $ Q_2 $ is chi-square distributed with $ k-1 $ dof.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{RFT: Both categorical}
	Both categorical variables with $ k $ and $ r $ categories.


	$$ Q_3(\bm{x}, \bm{y}) = \frac{1}{n} (d \times \hat{\Sigma}_d^{-1} \times d^T) $$

	If $ X \ci Y | Z $, then asymptotically $ Q_1(\bm{x}, \bm{y}) \sim
	\chi^2((k-1)(r-1)) $.

	\begin{itemize}
		\item Same as last, $ Q_3(\bm{x}, \bm{y}) $ is chi square distributed 
			with $ (k-1)(r-1) $ dof.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Test summary / Algorithm}
	\begin{itemize}
		\item If either $ X $ or $ Y $ are non-binary categorical,
			dummy/one-hot encode them.
		\item Train two random forest classifiers $ C_x = X \sim \bm{Z} $ and
			$ C_y = Y \sim \bm{Z} $
		\item Make predictions using these two classifiers $ \hat{X} =
			C_x(\bm{Z}) $ and $ \hat{Y} = C_y(\bm{Z}) $.
		\item Compute LS-Residuals $ R_{x|z} $ and $ R_{y|z} $ using $
			\hat{X} $ and $ \hat{Y} $.
		\item Compute covariance matrix over the residuals and compute the
			test statistic and degrees of freedom(dof).
		\item Use test statistic with dof to get the p-value.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Properties of RFT}
	\begin{itemize}
		\item Simple to implement
		\item Interpretable chi-square test statistic
		\item Symmetric by construction
		\item Computationally feasible
	\end{itemize}
\end{frame}

\section{Empirical Results}

\begin{frame}
	\frametitle{Empirical Analysis: Calibration}
	\begin{figure}
		\centering
		\includegraphics[scale=0.8]{imgs/calibration_add_vars.pdf}
		\caption*{Type I error vs significance level for sample sizes (top to
		bottom): $ [20, 40, 80] $ and number of conditional variables (left to
		right): $ [1, 3, 5] $ on conditionally independent simulated binary
		datasets.}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Empirical Analysis: Discrimination}
	\begin{figure}
		\centering
		\includegraphics{imgs/accuracy.pdf}
		\caption*{Accuracy (shading: mean $\pm$ standard error, $N=200$)
		of classifying simulated binary datasets (sample size: $1000$)
		as conditionally dependent or independent.}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Empirical Analysis: Discrimination}
	\begin{figure}
		\centering
		\includegraphics{imgs/accuracy_ordinal.pdf}
		\caption*{Accuracy (shading: mean $\pm$ standard error) of
		classifying simulated ordinal data (8 levels per variable) as
		conditionally dependent or independent.}	
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Applications: Model testing}
	\begin{figure}
		\centering
		\includegraphics{imgs/model_testing.pdf}
		\caption*{Precision and recall (shading: mean $\pm$ standard
		error) of testing implied CIs and equal number of randomly
		generated CIs in binary datasets (sample size: $1000$)
		simulated from random DAGs on $ 20 $ variables.}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Applications: Structure Learning}
	\begin{figure}
		\centering
		\includegraphics{imgs/sl_density.pdf}
		\caption*{Structure learning on simulated data. F1-score
		(shading: mean $\pm$ standard error) of the learned model
		skeletons for randomly generated DAGs with $20$ variables and
		varying edge probabilities.  Binary datasets of $ 1000 $
		samples are simulated from the DAGs using logistic models with
		all coefficients set to $ 0.15$.}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Applications: Structure Learning}
	\begin{figure}
		\centering
		\includegraphics{imgs/sl.pdf}
		\caption*{Structure learning on (a) ``alarm'', and (b)
		``insurance'' datasets.  F1-score (shading: mean $\pm$ standard
		error, $N=10$) of the learned model skeletons.  Presence of an
		edge is considered the "positive" case for computing the
		F1-Scores.}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Applications: Structure Learning}
	\begin{figure}
		\centering
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics[scale=0.85]{imgs/sl-adult-rf.pdf}
			\caption*{}
			\label{fig:sl_adult_model}
		\end{subfigure}%
		\begin{subfigure}{0.5\textwidth}
			\centering
			\includegraphics{imgs/adult_F1.pdf}
			\caption*{}
			\label{fig:sl_adult}
		\end{subfigure}
		\caption*{Structure learning on US census income dataset. (a)
		Learnt skeleton using RFT. (b) F1-score (shading: mean $\pm$
		standard error, $N=10$) when comparing $d$-connected variable
		pairs from the CPDAG to correlated variable pairs in the
		dataset.}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Runtime Analysis}
	\begin{figure}
		\centering
		\includegraphics{imgs/runtime.pdf}
		\caption*{Runtime (shading: mean $\pm$ standard error, $N=100$)
		for CI tests with varying numbers of conditional variables and
		$1000$ samples per dataset.
		}
	\end{figure}
\end{frame}

\section{Conclusion}
\begin{frame}
	\frametitle{Conclusion}
	\begin{itemize}
		\item A simple test which works reasonably well for low
			conditional variable cases and works better than other
			for high conditional variables.
		\item Can be extended to continuous variables giving a unified
			test for working with combinations of variable types.
		\item A hybrid approach with mc-mi and rft.
	\end{itemize}
\end{frame}

% \begin{frame}
% 	Questions / Suggestions
% \end{frame}

\end{document}
