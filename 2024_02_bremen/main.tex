\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usecolortheme{beaver}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{listings}
\usepackage{ragged2e}
\usepackage{titlecaps}
\usepackage{fancyvrb}

\def\ci{\perp\!\!\!\!\!\perp}

\newtheorem{proposition}{Proposition}
\Addlcwords{for a is but and with of in as the etc on to if}

\setbeamertemplate{section in toc}{\inserttocsectionnumber.~\inserttocsection}
\usetheme{Boadilla}
\makeatletter
\setbeamertemplate{footline}{%
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
            \usebeamerfont{author in head/foot}\insertshortauthor\expandafter\beamer@ifempty\expandafter{\beamer@shortinstitute}{}{~~(\insertshortinstitute)}
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.55\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.15\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
            \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
            \insertframenumber{} / \inserttotalframenumber\hspace*{2ex} 
        \end{beamercolorbox}}%
        \vskip0pt%
    }
\makeatother

\begin{document}

\title[]{Canonical Correlation Based Mixed Data Conditional Independence Testing: Empirical Analysis}
\author{Ankur Ankan}
\date{}

\maketitle

\begin{frame}{CI Tests}
	\begin{enumerate}
		\item Mutual Information based test (MI) \footnotemark
			\begin{itemize}
				\item Used as baseline.
				\item Equivalent to chi-squared test for discrete variables.
				\item Ordinal variables are treated as categorical.
			\end{itemize}
		\item Likelihood Ratio Test (MXM) \footnotemark
			\begin{itemize}
				\item Compares the likelihood of $ X \sim \bm{Z} $ and $ X \sim \bm{Z} + Y $.
				\item Regression method chosen based on data type.
			\end{itemize}
		\item Hotelling's $T^2$ test on the residuals from $ X \sim \mathbf{Z} $ and $ Y \sim \mathbf{Z} $. \footnotemark 
			\begin{itemize}
				\item Random Forest is used as the estimator for the analysis.
			\end{itemize}
		\item Pillai's Trace on the residuals from $ X \sim \mathbf{Z} $ and $ Y \sim \mathbf{Z} $.
			\begin{itemize}
				\item Random Forest is used as the estimator for the analysis.
			\end{itemize}
	\end{enumerate}

\footnotetext[1]{Scutari, Marco, and Robert Ness. bnlearn: Bayesian network structure learning, parameter learning and inference.}
\footnotetext[2]{Michail Tsagris, Giorgos Borboudakis, Vincenzo Lagani, and Ioannis Tsamardinos. Constraint-based causal discovery with mixed data.}
\footnotetext[3]{Ankan, Ankur, and Johannes Textor. A simple unified approach to testing high-dimensional conditional independences for categorical and ordinal data}

\end{frame}

\begin{frame}{Hotelling's $ T^2 $ on Residuals}

Given two residual matrices, $ R_{\mathbf{x}}: \mathbf{x} \sim \mathbf{z} $ and $ R_{\mathbf{y}}: \mathbf{y} \sim \mathbf{z} $, the test statistic is given as:

\begin{equation}
	Q(\mathbf{x}, \mathbf{y}) = \frac{1}{n} \left( d \times \hat{\Sigma}_d \times d^T \right)
\end{equation}
where,
\begin{eqnarray*}
d &  =  & (R_{\mathbb{I}(\mathbf{x}=1)} \cdot R_{\mathbb{I}(\mathbf{y}=1)}, \, \ldots \ ,
R_{\mathbb{I}(\mathbf{x}=k-1)} R_{\mathbb{I}(\mathbf{y}=1)}, \, \ldots \, ,
\\
 &  & R_{\mathbb{I}(\mathbf{x}=1)} \cdot R_{\mathbb{I}(\mathbf{y}=r-1)}, \, \ldots \ ,
R_{\mathbb{I}(\mathbf{x}=k-1)} R_{\mathbb{I}(\mathbf{y}=r-1)}
) \\
\hat{\Sigma}_d & = &\textrm{cov}(R_\mathbf{x} \odot R_\mathbf{y})
\end{eqnarray*}

% \begin{equation}
% 	\begin{split}
% 		d &= (R_{\mathbb{I}(\mathbf{x}=1)} \cdot R_{(\mathbf{y}=1)}, \, \ldots \ , R_{\mathbb{I}(\mathbf{x}=C_x)} \cdot R_{(\mathbf{y}=C_y)} ) \\
% 		\hat{\Sigma}_d &= \textrm{cov}(d)
% 	\end{split}
% \end{equation}



\vspace{1em}

\begin{itemize}
	\item Under null, $ Q $ is asymptotically $ \chi^2 (\mid d \mid) $ distributed.
	\item Hotelling's $ T^2 $ is a multi-dimensional location test.
	\item Test dimensionality is quadratic in the cardinality of categorical variables.
\end{itemize}

\end{frame}

\begin{frame}{Pillai's Trace on Residuals}
Given two residual matrices, $ R_{\mathbf{x}}: \mathbf{x} \sim \mathbf{z} $ and $ R_{\mathbf{y}}: \mathbf{y} \sim \mathbf{z} $, the test statistic is given as:

\begin{equation}
	\textit{V}(\mathbf{x}, \mathbf{y}) = \sum_{\rho \in \bm{\rho}(R_\mathbf{x}, R_\mathbf{y})} \rho^2
\end{equation}
where, $ \bm{\rho}({R_\mathbf{x}, R_\mathbf{y}}) $  is the canonical correlations between $ R_\mathbf{x} $ and $ R_\mathbf{y} $.

\vspace{1em}

\begin{itemize}
	\item No exact asymptotic distribution of $ V $ is known, but p-values can be computed using F-approximations. \footnotemark
	\item Test dimensionality is linear in the cardinality of categorical variables.
\end{itemize}

\footnotetext[4]{Muller, Keith E., and Bercedis L. Peterson. "Practical methods for computing power in testing the multivariate general linear hypothesis."}
\end{frame}

\begin{frame}{Simulating Mixed Data}
	Given a node $ Y $ with parents $ Pa_Y $, $ Y $ is simulated as follows:
	\begin{enumerate}
		\item Categorical variables in $ Pa_Y $ are dummy encoded.
		\item Ordinal variables in $Pa_Y$ are treated as continuous.
		\item \begin{itemize}
			\item If $ Y $ is continuous, a simple linear regression is used.
			% \item If $ Y $ is continuous, $ Y = \mathbf{B} Pa_Y + \epsilon $
			\item If $ Y $ is categorical, a multinomial logistic regression is used.
			% \item If $ Y $ is categorical, $ Y \sim \lambda(\mathbf{B} Pa_Y + \epsilon) $, where $ \lambda(x) = \frac{e^x}{1+e^x} $.
			\item If $ Y $ is ordinal, a proportional odds logistic regression is used.
			% \item If $ Y $ is ordinal, using proportional odds logistic regression. $ Y \sim \lambda(A + \mathbf{B} Pa_Y + \epsilon) $.
		\end{itemize}
	\end{enumerate}
\end{frame}

\begin{frame}{Calibration Analysis}

\begin{center}
\begin{tikzpicture}[yscale=.75]
	\node (x) at (-1,1) {$X$};
	\node (y) at (3,1) {$Y$};
	\node (z) at (1,2) {$Z_1$};
	\node (z2) at (1, 2.5) {$Z_2$};
	\node (dots) at (1, 3) {$ \vdots $};
	\node (zk) at (1, 3.5) {$ Z_k $};
	\draw [->] (z) -- (y);
	\draw [->] (z) -- (x);
	\draw [->] (z2) -- (x);
	\draw [->] (z2) -- (y);
	\draw [->] (zk) -- (x);
	\draw [->] (zk) -- (y);
\end{tikzpicture}
\end{center}

\begin{itemize}
	\item Conditionally independent datasets are simulated.
	\item Under null (i.e. conditional independence), the p-values should
		have a uniform distribution.
	\item All effects are fixed to $ 0.2 $.
\end{itemize}

\end{frame}

\begin{frame}{Calibration: Mixed Data}
	\begin{figure}[t]
		\centering
		\includegraphics{../../2023-canonical-cor/code/plots/calibration_mixed.pdf}
		\caption*{\footnotesize{Left to Right: Increasing number of conditional variables [2, 4, 8]. Top to Bottom: Increasing sample size: [40, 60, 80]. Cardinality of categorical variables is fixed to $ 4 $.}}
	\end{figure}
\end{frame}

\begin{frame}{Calibration: Categorical Data}
	\begin{figure}[t]
		\centering
		\includegraphics{../../2023-canonical-cor/code/plots/calibration.pdf}
		\caption*{\footnotesize{Left to Right: Increasing cardinality of categorical variables [4, 8, 12]. Top to Bottom: Increasing sample size: [40, 60, 80]. No. of conditional variables is fixed to $ 4 $.}}
	\end{figure}
\end{frame}

\begin{frame}{Discrimination Analysis}

\begin{center}
\begin{tikzpicture}[yscale=.75]
	\node (x) at (-1,0) {$X$};
	\node (y) at (3,0) {$Y$};
	\node (z) at (1,1) {$Z_1$};
	\node (z2) at (1, 1.5) {$Z_2$};
	\node  at (1, 2) {$ \vdots $};
	\node (zk) at (1, 2.5) {$Z_k$};
	\draw [->] (x) edge node [midway, below] {$\beta$} (y);
	\draw [->] (z) -- (y);
	\draw [->] (z) -- (x);
	\draw [->] (z2) -- (y);
	\draw [->] (z2) -- (x);
	\draw [->] (zk) -- (y);
	\draw [->] (zk) -- (x);
\end{tikzpicture}
\end{center}

\begin{itemize}
	\item Equal number of conditionally dependent and independent datasets are generated, and the accuracy of tests to correctly classify them is measured.
	\item Conditionally dependent datasets are generated with varying effect $ \beta $.
	\item Conditionally independent datasets are generated by setting $ \beta = 0 $.
	\item All other parameters are fixed to $ 0.2 $. Sample size fixed to $ 1000 $.
\end{itemize}

\end{frame}

\begin{frame}{Discrimination: Mixed Data}
	\begin{figure}[t]
		\centering
		\includegraphics{../../2023-canonical-cor/code/plots/accuracy_mixed.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Discrimination: Categorical Data}
	\begin{figure}[t]
		\centering
		\includegraphics{../../2023-canonical-cor/code/plots/accuracy_cat.pdf}
	\end{figure}
\end{frame}


% \begin{frame}{Structure Learning: Data Generation}
% 	DAG generation mechanism.
% \end{frame}
% 
% \begin{frame}{Structure Learning: Mixed Data}
% \end{frame}
% 
% \begin{frame}{Structure Learning: Categorical Data}
% \end{frame}

\begin{frame}{Conclusion}
	\begin{itemize}
		\item The chi-squared or mutual information based test fails in most high dimensional scenarios.
		\item Canonical correlations based Pillai's Trace is better
			calibrated than other tests especially in
			high-dimensional scenarios.
		\item The power of all the tests is comparable (except MI).
		\item Would be interesting to compare in non-linear settings as
			residualization approaches can use estimators that can
			deal with non-linearity.
	\end{itemize}
\end{frame}

\end{document}
